{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>non_hashtags_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512581807853633541</td>\n",
       "      <td>RT @forever_seesaw: #PTDinLV Do not Uber/Lyft ...</td>\n",
       "      <td>RT @forever_seesaw: &lt;UNKTAG&gt; Do not Uber/Lyft ...</td>\n",
       "      <td>162180146</td>\n",
       "      <td>[PTDinLV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512581787075026944</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>952923842363645952</td>\n",
       "      <td>[BTS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512581771153461248</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>1370022606632484867</td>\n",
       "      <td>[BTS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512581733543120896</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>1285244583228502017</td>\n",
       "      <td>[PTD_ON_STAGE_LV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512581724718661632</td>\n",
       "      <td>RT @URDailyHistory: 8 April 1974: #Atlanta Bra...</td>\n",
       "      <td>RT @URDailyHistory: 8 April 1974: &lt;UNKTAG&gt; Bra...</td>\n",
       "      <td>1126271645000163328</td>\n",
       "      <td>[Atlanta, outfielder, Aaron]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1512581807853633541  RT @forever_seesaw: #PTDinLV Do not Uber/Lyft ...   \n",
       "1  1512581787075026944  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "2  1512581771153461248  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "3  1512581733543120896  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...   \n",
       "4  1512581724718661632  RT @URDailyHistory: 8 April 1974: #Atlanta Bra...   \n",
       "\n",
       "                                   non_hashtags_text            author_id  \\\n",
       "0  RT @forever_seesaw: <UNKTAG> Do not Uber/Lyft ...            162180146   \n",
       "1  RT @FOX5Vegas: <UNKTAG> mania has completely t...   952923842363645952   \n",
       "2  RT @FOX5Vegas: <UNKTAG> mania has completely t...  1370022606632484867   \n",
       "3  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...  1285244583228502017   \n",
       "4  RT @URDailyHistory: 8 April 1974: <UNKTAG> Bra...  1126271645000163328   \n",
       "\n",
       "                       hashtags  \n",
       "0                     [PTDinLV]  \n",
       "1                         [BTS]  \n",
       "2                         [BTS]  \n",
       "3             [PTD_ON_STAGE_LV]  \n",
       "4  [Atlanta, outfielder, Aaron]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tagged_tweets.csv')\n",
    "df.hashtags = df.hashtags.apply(eval)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/34980 [00:00<?, ?it/s]C:\\Users\\10213\\AppData\\Local\\Temp/ipykernel_28404/3716890117.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['hashtags'].loc[i] = df['hashtags'].loc[i][0]\n",
      "100%|██████████| 34980/34980 [04:37<00:00, 125.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>non_hashtags_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512581807853633541</td>\n",
       "      <td>RT @forever_seesaw: #PTDinLV Do not Uber/Lyft ...</td>\n",
       "      <td>RT @forever_seesaw: &lt;UNKTAG&gt; Do not Uber/Lyft ...</td>\n",
       "      <td>162180146</td>\n",
       "      <td>PTDinLV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512581787075026944</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>952923842363645952</td>\n",
       "      <td>BTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512581771153461248</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>1370022606632484867</td>\n",
       "      <td>BTS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512581733543120896</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>1285244583228502017</td>\n",
       "      <td>PTD_ON_STAGE_LV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512581724718661632</td>\n",
       "      <td>RT @URDailyHistory: 8 April 1974: #Atlanta Bra...</td>\n",
       "      <td>RT @URDailyHistory: 8 April 1974: &lt;UNKTAG&gt; Bra...</td>\n",
       "      <td>1126271645000163328</td>\n",
       "      <td>Atlanta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1512581807853633541  RT @forever_seesaw: #PTDinLV Do not Uber/Lyft ...   \n",
       "1  1512581787075026944  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "2  1512581771153461248  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "3  1512581733543120896  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...   \n",
       "4  1512581724718661632  RT @URDailyHistory: 8 April 1974: #Atlanta Bra...   \n",
       "\n",
       "                                   non_hashtags_text            author_id  \\\n",
       "0  RT @forever_seesaw: <UNKTAG> Do not Uber/Lyft ...            162180146   \n",
       "1  RT @FOX5Vegas: <UNKTAG> mania has completely t...   952923842363645952   \n",
       "2  RT @FOX5Vegas: <UNKTAG> mania has completely t...  1370022606632484867   \n",
       "3  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...  1285244583228502017   \n",
       "4  RT @URDailyHistory: 8 April 1974: <UNKTAG> Bra...  1126271645000163328   \n",
       "\n",
       "          hashtags  \n",
       "0          PTDinLV  \n",
       "1              BTS  \n",
       "2              BTS  \n",
       "3  PTD_ON_STAGE_LV  \n",
       "4          Atlanta  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in tqdm(range(len(df))):\n",
    "    df['hashtags'].loc[i] = df['hashtags'].loc[i][0]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BTS                461\n",
       "Ukraine            342\n",
       "Tigray             335\n",
       "PTD_ON_STAGE_LV    288\n",
       "BBMAs              226\n",
       "                  ... \n",
       "Guardian             1\n",
       "Protospiel           1\n",
       "ワンインチチャンネル           1\n",
       "위아이                  1\n",
       "Dreamcatcher         1\n",
       "Name: hashtags, Length: 7062, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.hashtags.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7062/7062 [00:22<00:00, 308.69it/s]\n",
      "  0%|          | 0/34980 [00:00<?, ?it/s]C:\\Users\\10213\\AppData\\Local\\Temp/ipykernel_28404/236368660.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['class'].loc[i] = vocabulary[df['hashtags'].loc[i]]\n",
      "100%|██████████| 34980/34980 [01:31<00:00, 383.74it/s]\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "decoder = {}\n",
    "frequency = {}\n",
    "cnt = 0\n",
    "for tags in tqdm(df.hashtags.value_counts().index):\n",
    "    if df.hashtags.value_counts()[tags] > 20:\n",
    "        frequency[tags] = df.hashtags.value_counts()[tags]\n",
    "        vocabulary[tags] = cnt\n",
    "        decoder[cnt] = tags\n",
    "        cnt += 1\n",
    "df['class'] = -1\n",
    "for i in tqdm(range(len(df))):\n",
    "    if df['hashtags'].loc[i] in vocabulary:\n",
    "        df['class'].loc[i] = vocabulary[df['hashtags'].loc[i]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>non_hashtags_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1512581807853633541</td>\n",
       "      <td>RT @forever_seesaw: #PTDinLV Do not Uber/Lyft ...</td>\n",
       "      <td>RT @forever_seesaw: &lt;UNKTAG&gt; Do not Uber/Lyft ...</td>\n",
       "      <td>162180146</td>\n",
       "      <td>PTDinLV</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512581787075026944</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>952923842363645952</td>\n",
       "      <td>BTS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512581771153461248</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>1370022606632484867</td>\n",
       "      <td>BTS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512581733543120896</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>1285244583228502017</td>\n",
       "      <td>PTD_ON_STAGE_LV</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1512581724718661632</td>\n",
       "      <td>RT @URDailyHistory: 8 April 1974: #Atlanta Bra...</td>\n",
       "      <td>RT @URDailyHistory: 8 April 1974: &lt;UNKTAG&gt; Bra...</td>\n",
       "      <td>1126271645000163328</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "0  1512581807853633541  RT @forever_seesaw: #PTDinLV Do not Uber/Lyft ...   \n",
       "1  1512581787075026944  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "2  1512581771153461248  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "3  1512581733543120896  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...   \n",
       "4  1512581724718661632  RT @URDailyHistory: 8 April 1974: #Atlanta Bra...   \n",
       "\n",
       "                                   non_hashtags_text            author_id  \\\n",
       "0  RT @forever_seesaw: <UNKTAG> Do not Uber/Lyft ...            162180146   \n",
       "1  RT @FOX5Vegas: <UNKTAG> mania has completely t...   952923842363645952   \n",
       "2  RT @FOX5Vegas: <UNKTAG> mania has completely t...  1370022606632484867   \n",
       "3  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...  1285244583228502017   \n",
       "4  RT @URDailyHistory: 8 April 1974: <UNKTAG> Bra...  1126271645000163328   \n",
       "\n",
       "          hashtags  class  \n",
       "0          PTDinLV     -1  \n",
       "1              BTS      0  \n",
       "2              BTS      0  \n",
       "3  PTD_ON_STAGE_LV      3  \n",
       "4          Atlanta     -1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ukraine'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[df[\"class\"] != -1].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>non_hashtags_text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1512581787075026944</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>952923842363645952</td>\n",
       "      <td>BTS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1512581771153461248</td>\n",
       "      <td>RT @FOX5Vegas: #BTS mania has completely taken...</td>\n",
       "      <td>RT @FOX5Vegas: &lt;UNKTAG&gt; mania has completely t...</td>\n",
       "      <td>1370022606632484867</td>\n",
       "      <td>BTS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1512581733543120896</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...</td>\n",
       "      <td>1285244583228502017</td>\n",
       "      <td>PTD_ON_STAGE_LV</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1512581722931671042</td>\n",
       "      <td>RT @Army_Connect: First thing we saw upon arri...</td>\n",
       "      <td>RT @Army_Connect: First thing we saw upon arri...</td>\n",
       "      <td>1079502652835282944</td>\n",
       "      <td>PTD_ON_STAGE_LV</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1512581703298334720</td>\n",
       "      <td>RT @sunflowercharts: .@BTS_twt เตรียมจัดคอนเสิ...</td>\n",
       "      <td>RT @sunflowercharts: .@BTS_twt เตรียมจัดคอนเสิ...</td>\n",
       "      <td>1270265403739004929</td>\n",
       "      <td>BTS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                               text  \\\n",
       "1  1512581787075026944  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "2  1512581771153461248  RT @FOX5Vegas: #BTS mania has completely taken...   \n",
       "3  1512581733543120896  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...   \n",
       "5  1512581722931671042  RT @Army_Connect: First thing we saw upon arri...   \n",
       "6  1512581703298334720  RT @sunflowercharts: .@BTS_twt เตรียมจัดคอนเสิ...   \n",
       "\n",
       "                                   non_hashtags_text            author_id  \\\n",
       "1  RT @FOX5Vegas: <UNKTAG> mania has completely t...   952923842363645952   \n",
       "2  RT @FOX5Vegas: <UNKTAG> mania has completely t...  1370022606632484867   \n",
       "3  Army walk to stadiam. 💜 คนเดินข้ามสะพานเพื่อมา...  1285244583228502017   \n",
       "5  RT @Army_Connect: First thing we saw upon arri...  1079502652835282944   \n",
       "6  RT @sunflowercharts: .@BTS_twt เตรียมจัดคอนเสิ...  1270265403739004929   \n",
       "\n",
       "          hashtags  class  \n",
       "1              BTS      0  \n",
       "2              BTS      0  \n",
       "3  PTD_ON_STAGE_LV      3  \n",
       "5  PTD_ON_STAGE_LV      3  \n",
       "6              BTS      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11793"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11793it [00:00, 13664.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "drop_rows = []\n",
    "num_totalTag = len(data)\n",
    "text_list = []\n",
    "tag_list = []\n",
    "class_list = []\n",
    "for index, row in tqdm(data.iterrows()):\n",
    "    # sampling\n",
    "    z = frequency[row[-2]] / num_totalTag\n",
    "    p = ((z / 0.001)**0.5 + 1) * (0.001 / z)\n",
    "    for j in range(10):\n",
    "        outcome = random.random() # a random number from 0 to 1.\n",
    "                \n",
    "        if outcome <= p:\n",
    "            text_list.append(row[-4])\n",
    "            tag_list.append(row[-2])\n",
    "            class_list.append(row[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BTS                 850\n",
       "Ukraine             782\n",
       "Tigray              696\n",
       "Bitcoin             672\n",
       "BBMAs               661\n",
       "                   ... \n",
       "Justiceforbella     210\n",
       "quote               210\n",
       "Wishlist            210\n",
       "Siguemeytesigo      210\n",
       "SonicTheHedgehog    210\n",
       "Name: hashtags, Length: 210, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.DataFrame({\"non_hashtags_text\":text_list,\"hashtags\":tag_list,\"class\":class_list})\n",
    "data2.hashtags.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71561"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# ----- 1. Preprocess data -----#\n",
    "# Preprocess data\n",
    "X = list(data2[\"non_hashtags_text\"])\n",
    "y = list(data2[\"class\"])\n",
    "# X_val = list(data[\"non_hashtags_text\"][10000:11000])\n",
    "# y_val = list(data[\"class\"][10000:11000])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_test[:1000]\n",
    "y_val = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\10213/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at C:\\Users\\10213/.cache\\huggingface\\transformers\\45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at C:\\Users\\10213/.cache\\huggingface\\transformers\\c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\10213/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Using eos_token, but it is not set yet.\n",
      "Assigning [PAD] to the pad_token key of the tokenizer\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at C:\\Users\\10213/.cache\\huggingface\\transformers\\3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\",\n",
      "    \"66\": \"LABEL_66\",\n",
      "    \"67\": \"LABEL_67\",\n",
      "    \"68\": \"LABEL_68\",\n",
      "    \"69\": \"LABEL_69\",\n",
      "    \"70\": \"LABEL_70\",\n",
      "    \"71\": \"LABEL_71\",\n",
      "    \"72\": \"LABEL_72\",\n",
      "    \"73\": \"LABEL_73\",\n",
      "    \"74\": \"LABEL_74\",\n",
      "    \"75\": \"LABEL_75\",\n",
      "    \"76\": \"LABEL_76\",\n",
      "    \"77\": \"LABEL_77\",\n",
      "    \"78\": \"LABEL_78\",\n",
      "    \"79\": \"LABEL_79\",\n",
      "    \"80\": \"LABEL_80\",\n",
      "    \"81\": \"LABEL_81\",\n",
      "    \"82\": \"LABEL_82\",\n",
      "    \"83\": \"LABEL_83\",\n",
      "    \"84\": \"LABEL_84\",\n",
      "    \"85\": \"LABEL_85\",\n",
      "    \"86\": \"LABEL_86\",\n",
      "    \"87\": \"LABEL_87\",\n",
      "    \"88\": \"LABEL_88\",\n",
      "    \"89\": \"LABEL_89\",\n",
      "    \"90\": \"LABEL_90\",\n",
      "    \"91\": \"LABEL_91\",\n",
      "    \"92\": \"LABEL_92\",\n",
      "    \"93\": \"LABEL_93\",\n",
      "    \"94\": \"LABEL_94\",\n",
      "    \"95\": \"LABEL_95\",\n",
      "    \"96\": \"LABEL_96\",\n",
      "    \"97\": \"LABEL_97\",\n",
      "    \"98\": \"LABEL_98\",\n",
      "    \"99\": \"LABEL_99\",\n",
      "    \"100\": \"LABEL_100\",\n",
      "    \"101\": \"LABEL_101\",\n",
      "    \"102\": \"LABEL_102\",\n",
      "    \"103\": \"LABEL_103\",\n",
      "    \"104\": \"LABEL_104\",\n",
      "    \"105\": \"LABEL_105\",\n",
      "    \"106\": \"LABEL_106\",\n",
      "    \"107\": \"LABEL_107\",\n",
      "    \"108\": \"LABEL_108\",\n",
      "    \"109\": \"LABEL_109\",\n",
      "    \"110\": \"LABEL_110\",\n",
      "    \"111\": \"LABEL_111\",\n",
      "    \"112\": \"LABEL_112\",\n",
      "    \"113\": \"LABEL_113\",\n",
      "    \"114\": \"LABEL_114\",\n",
      "    \"115\": \"LABEL_115\",\n",
      "    \"116\": \"LABEL_116\",\n",
      "    \"117\": \"LABEL_117\",\n",
      "    \"118\": \"LABEL_118\",\n",
      "    \"119\": \"LABEL_119\",\n",
      "    \"120\": \"LABEL_120\",\n",
      "    \"121\": \"LABEL_121\",\n",
      "    \"122\": \"LABEL_122\",\n",
      "    \"123\": \"LABEL_123\",\n",
      "    \"124\": \"LABEL_124\",\n",
      "    \"125\": \"LABEL_125\",\n",
      "    \"126\": \"LABEL_126\",\n",
      "    \"127\": \"LABEL_127\",\n",
      "    \"128\": \"LABEL_128\",\n",
      "    \"129\": \"LABEL_129\",\n",
      "    \"130\": \"LABEL_130\",\n",
      "    \"131\": \"LABEL_131\",\n",
      "    \"132\": \"LABEL_132\",\n",
      "    \"133\": \"LABEL_133\",\n",
      "    \"134\": \"LABEL_134\",\n",
      "    \"135\": \"LABEL_135\",\n",
      "    \"136\": \"LABEL_136\",\n",
      "    \"137\": \"LABEL_137\",\n",
      "    \"138\": \"LABEL_138\",\n",
      "    \"139\": \"LABEL_139\",\n",
      "    \"140\": \"LABEL_140\",\n",
      "    \"141\": \"LABEL_141\",\n",
      "    \"142\": \"LABEL_142\",\n",
      "    \"143\": \"LABEL_143\",\n",
      "    \"144\": \"LABEL_144\",\n",
      "    \"145\": \"LABEL_145\",\n",
      "    \"146\": \"LABEL_146\",\n",
      "    \"147\": \"LABEL_147\",\n",
      "    \"148\": \"LABEL_148\",\n",
      "    \"149\": \"LABEL_149\",\n",
      "    \"150\": \"LABEL_150\",\n",
      "    \"151\": \"LABEL_151\",\n",
      "    \"152\": \"LABEL_152\",\n",
      "    \"153\": \"LABEL_153\",\n",
      "    \"154\": \"LABEL_154\",\n",
      "    \"155\": \"LABEL_155\",\n",
      "    \"156\": \"LABEL_156\",\n",
      "    \"157\": \"LABEL_157\",\n",
      "    \"158\": \"LABEL_158\",\n",
      "    \"159\": \"LABEL_159\",\n",
      "    \"160\": \"LABEL_160\",\n",
      "    \"161\": \"LABEL_161\",\n",
      "    \"162\": \"LABEL_162\",\n",
      "    \"163\": \"LABEL_163\",\n",
      "    \"164\": \"LABEL_164\",\n",
      "    \"165\": \"LABEL_165\",\n",
      "    \"166\": \"LABEL_166\",\n",
      "    \"167\": \"LABEL_167\",\n",
      "    \"168\": \"LABEL_168\",\n",
      "    \"169\": \"LABEL_169\",\n",
      "    \"170\": \"LABEL_170\",\n",
      "    \"171\": \"LABEL_171\",\n",
      "    \"172\": \"LABEL_172\",\n",
      "    \"173\": \"LABEL_173\",\n",
      "    \"174\": \"LABEL_174\",\n",
      "    \"175\": \"LABEL_175\",\n",
      "    \"176\": \"LABEL_176\",\n",
      "    \"177\": \"LABEL_177\",\n",
      "    \"178\": \"LABEL_178\",\n",
      "    \"179\": \"LABEL_179\",\n",
      "    \"180\": \"LABEL_180\",\n",
      "    \"181\": \"LABEL_181\",\n",
      "    \"182\": \"LABEL_182\",\n",
      "    \"183\": \"LABEL_183\",\n",
      "    \"184\": \"LABEL_184\",\n",
      "    \"185\": \"LABEL_185\",\n",
      "    \"186\": \"LABEL_186\",\n",
      "    \"187\": \"LABEL_187\",\n",
      "    \"188\": \"LABEL_188\",\n",
      "    \"189\": \"LABEL_189\",\n",
      "    \"190\": \"LABEL_190\",\n",
      "    \"191\": \"LABEL_191\",\n",
      "    \"192\": \"LABEL_192\",\n",
      "    \"193\": \"LABEL_193\",\n",
      "    \"194\": \"LABEL_194\",\n",
      "    \"195\": \"LABEL_195\",\n",
      "    \"196\": \"LABEL_196\",\n",
      "    \"197\": \"LABEL_197\",\n",
      "    \"198\": \"LABEL_198\",\n",
      "    \"199\": \"LABEL_199\",\n",
      "    \"200\": \"LABEL_200\",\n",
      "    \"201\": \"LABEL_201\",\n",
      "    \"202\": \"LABEL_202\",\n",
      "    \"203\": \"LABEL_203\",\n",
      "    \"204\": \"LABEL_204\",\n",
      "    \"205\": \"LABEL_205\",\n",
      "    \"206\": \"LABEL_206\",\n",
      "    \"207\": \"LABEL_207\",\n",
      "    \"208\": \"LABEL_208\",\n",
      "    \"209\": \"LABEL_209\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_100\": 100,\n",
      "    \"LABEL_101\": 101,\n",
      "    \"LABEL_102\": 102,\n",
      "    \"LABEL_103\": 103,\n",
      "    \"LABEL_104\": 104,\n",
      "    \"LABEL_105\": 105,\n",
      "    \"LABEL_106\": 106,\n",
      "    \"LABEL_107\": 107,\n",
      "    \"LABEL_108\": 108,\n",
      "    \"LABEL_109\": 109,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_110\": 110,\n",
      "    \"LABEL_111\": 111,\n",
      "    \"LABEL_112\": 112,\n",
      "    \"LABEL_113\": 113,\n",
      "    \"LABEL_114\": 114,\n",
      "    \"LABEL_115\": 115,\n",
      "    \"LABEL_116\": 116,\n",
      "    \"LABEL_117\": 117,\n",
      "    \"LABEL_118\": 118,\n",
      "    \"LABEL_119\": 119,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_120\": 120,\n",
      "    \"LABEL_121\": 121,\n",
      "    \"LABEL_122\": 122,\n",
      "    \"LABEL_123\": 123,\n",
      "    \"LABEL_124\": 124,\n",
      "    \"LABEL_125\": 125,\n",
      "    \"LABEL_126\": 126,\n",
      "    \"LABEL_127\": 127,\n",
      "    \"LABEL_128\": 128,\n",
      "    \"LABEL_129\": 129,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_130\": 130,\n",
      "    \"LABEL_131\": 131,\n",
      "    \"LABEL_132\": 132,\n",
      "    \"LABEL_133\": 133,\n",
      "    \"LABEL_134\": 134,\n",
      "    \"LABEL_135\": 135,\n",
      "    \"LABEL_136\": 136,\n",
      "    \"LABEL_137\": 137,\n",
      "    \"LABEL_138\": 138,\n",
      "    \"LABEL_139\": 139,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_140\": 140,\n",
      "    \"LABEL_141\": 141,\n",
      "    \"LABEL_142\": 142,\n",
      "    \"LABEL_143\": 143,\n",
      "    \"LABEL_144\": 144,\n",
      "    \"LABEL_145\": 145,\n",
      "    \"LABEL_146\": 146,\n",
      "    \"LABEL_147\": 147,\n",
      "    \"LABEL_148\": 148,\n",
      "    \"LABEL_149\": 149,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_150\": 150,\n",
      "    \"LABEL_151\": 151,\n",
      "    \"LABEL_152\": 152,\n",
      "    \"LABEL_153\": 153,\n",
      "    \"LABEL_154\": 154,\n",
      "    \"LABEL_155\": 155,\n",
      "    \"LABEL_156\": 156,\n",
      "    \"LABEL_157\": 157,\n",
      "    \"LABEL_158\": 158,\n",
      "    \"LABEL_159\": 159,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_160\": 160,\n",
      "    \"LABEL_161\": 161,\n",
      "    \"LABEL_162\": 162,\n",
      "    \"LABEL_163\": 163,\n",
      "    \"LABEL_164\": 164,\n",
      "    \"LABEL_165\": 165,\n",
      "    \"LABEL_166\": 166,\n",
      "    \"LABEL_167\": 167,\n",
      "    \"LABEL_168\": 168,\n",
      "    \"LABEL_169\": 169,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_170\": 170,\n",
      "    \"LABEL_171\": 171,\n",
      "    \"LABEL_172\": 172,\n",
      "    \"LABEL_173\": 173,\n",
      "    \"LABEL_174\": 174,\n",
      "    \"LABEL_175\": 175,\n",
      "    \"LABEL_176\": 176,\n",
      "    \"LABEL_177\": 177,\n",
      "    \"LABEL_178\": 178,\n",
      "    \"LABEL_179\": 179,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_180\": 180,\n",
      "    \"LABEL_181\": 181,\n",
      "    \"LABEL_182\": 182,\n",
      "    \"LABEL_183\": 183,\n",
      "    \"LABEL_184\": 184,\n",
      "    \"LABEL_185\": 185,\n",
      "    \"LABEL_186\": 186,\n",
      "    \"LABEL_187\": 187,\n",
      "    \"LABEL_188\": 188,\n",
      "    \"LABEL_189\": 189,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_190\": 190,\n",
      "    \"LABEL_191\": 191,\n",
      "    \"LABEL_192\": 192,\n",
      "    \"LABEL_193\": 193,\n",
      "    \"LABEL_194\": 194,\n",
      "    \"LABEL_195\": 195,\n",
      "    \"LABEL_196\": 196,\n",
      "    \"LABEL_197\": 197,\n",
      "    \"LABEL_198\": 198,\n",
      "    \"LABEL_199\": 199,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_200\": 200,\n",
      "    \"LABEL_201\": 201,\n",
      "    \"LABEL_202\": 202,\n",
      "    \"LABEL_203\": 203,\n",
      "    \"LABEL_204\": 204,\n",
      "    \"LABEL_205\": 205,\n",
      "    \"LABEL_206\": 206,\n",
      "    \"LABEL_207\": 207,\n",
      "    \"LABEL_208\": 208,\n",
      "    \"LABEL_209\": 209,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_66\": 66,\n",
      "    \"LABEL_67\": 67,\n",
      "    \"LABEL_68\": 68,\n",
      "    \"LABEL_69\": 69,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_70\": 70,\n",
      "    \"LABEL_71\": 71,\n",
      "    \"LABEL_72\": 72,\n",
      "    \"LABEL_73\": 73,\n",
      "    \"LABEL_74\": 74,\n",
      "    \"LABEL_75\": 75,\n",
      "    \"LABEL_76\": 76,\n",
      "    \"LABEL_77\": 77,\n",
      "    \"LABEL_78\": 78,\n",
      "    \"LABEL_79\": 79,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_80\": 80,\n",
      "    \"LABEL_81\": 81,\n",
      "    \"LABEL_82\": 82,\n",
      "    \"LABEL_83\": 83,\n",
      "    \"LABEL_84\": 84,\n",
      "    \"LABEL_85\": 85,\n",
      "    \"LABEL_86\": 86,\n",
      "    \"LABEL_87\": 87,\n",
      "    \"LABEL_88\": 88,\n",
      "    \"LABEL_89\": 89,\n",
      "    \"LABEL_9\": 9,\n",
      "    \"LABEL_90\": 90,\n",
      "    \"LABEL_91\": 91,\n",
      "    \"LABEL_92\": 92,\n",
      "    \"LABEL_93\": 93,\n",
      "    \"LABEL_94\": 94,\n",
      "    \"LABEL_95\": 95,\n",
      "    \"LABEL_96\": 96,\n",
      "    \"LABEL_97\": 97,\n",
      "    \"LABEL_98\": 98,\n",
      "    \"LABEL_99\": 99\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at C:\\Users\\10213/.cache\\huggingface\\transformers\\a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer from hugging face\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel\n",
    "from transformers import AutoModelForMaskedLM, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# development = dev_df.dropna()\n",
    "\n",
    "# Define pretrained tokenizer and model\n",
    "# \"vinai/bertweet-base\",\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=False)\n",
    "tokenizer.mask_token = '<UNKTAG>'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(vocabulary))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=20)\n",
    "# y_train_tokenized = tokenizer(y_train, padding=True, truncation=True, max_length=50)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=20)   \n",
    "# y_val_tokenized = tokenizer(y_val, padding=True, truncation=True, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1030, 26319, 12199, 11431, 12184,  1045,   100,  2005,  5440,\n",
       "          2931,  3063,  2012,  1996,   100,  1052,   102,     0,     0,     0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]),\n",
       " 'labels': tensor(62)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create torch dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:    \n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "train_dataset.__getitem__(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28404/1949875224.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mload_best_model_at_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m )\n\u001b[1;32m---> 35\u001b[1;33m trainer = Trainer(\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;31m# Seed must be set before instantiating the model when using model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhp_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\transformers\\trainer_utils.py\u001b[0m in \u001b[0;36mset_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# ^^ safe to call this function even if cuda is not available\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\random.py\u001b[0m in \u001b[0;36mmanual_seed\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_in_bad_fork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\cuda\\random.py\u001b[0m in \u001b[0;36mmanual_seed_all\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    111\u001b[0m             \u001b[0mdefault_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0m_lazy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_call\u001b[1;34m(callable, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_lazy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;31m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\cuda\\random.py\u001b[0m in \u001b[0;36mcb\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mdefault_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_generators\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[0mdefault_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0m_lazy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed_all\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "# ----- 2. Fine-tune pretrained model -----#\n",
    "# Define Trainer parameters\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from transformers import EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "#     return {\"rmse\": rmse}\n",
    "\n",
    "\n",
    "def compute_metrics(p):    \n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "#     accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "#     recall = recall_score(y_true=labels, y_pred=pred)\n",
    "#     precision = precision_score(y_true=labels, y_pred=pred)\n",
    "#    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "    f1_mac = f1_score(y_true=labels, y_pred=pred, average=\"macro\")\n",
    "    return {\"f1_macro\": f1_mac}\n",
    "\n",
    "\n",
    "\n",
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)],\n",
    ")\n",
    "\n",
    "# Train pre-trained model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'not'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer(X_test[1000:], padding=True, truncation=True, max_length=20)\n",
    "Xevaluation = Dataset(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_28404/1907550624.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mraw_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXevaluation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# pred = raw_pred.squeeze(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mto\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    897\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m     def register_backward_hook(\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;31m# `with torch.no_grad():`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m                 \u001b[0mparam_applied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\softwares\\anaconda\\envs\\si630wn22\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    895\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[0;32m    896\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[1;32m--> 897\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "raw_pred, _, _ = trainer.predict(Xevaluation)\n",
    "# pred = raw_pred.squeeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.argmax(raw_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LABEL_4'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "task = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "task(\"Hello world\")[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21552/3162338711.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pred' is not defined"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i in pred:\n",
    "    \n",
    "    res.append(decoder[i])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for i in range(len(y_test[1000:])):\n",
    "    y_true.append(decoder[y_test[1000:][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID19',\n",
       " 'Sukhbaatar',\n",
       " 'SBM2022',\n",
       " 'Jimin',\n",
       " 'DAY6',\n",
       " 'GONZAGA',\n",
       " 'IndieGameDev',\n",
       " 'ToEvangeliseChrist',\n",
       " 'Fear',\n",
       " 'Crypto',\n",
       " 'Rockets',\n",
       " 'listing',\n",
       " 'EXO',\n",
       " 'OppositionHypocrisyExposed',\n",
       " 'رمضان_مميز_بايه',\n",
       " 'ScottyTheWorstPrimeMinisterEver',\n",
       " 'ICOMCodeOfEthics',\n",
       " 'TigerWoods',\n",
       " 'teamchais',\n",
       " 'fat',\n",
       " 'Fukushima',\n",
       " 'Binance',\n",
       " 'BTS',\n",
       " 'AFLWGF',\n",
       " 'KetanjiBrownJackson',\n",
       " 'Tigray',\n",
       " 'SaturdayVibes',\n",
       " 'HelluvaBoss',\n",
       " 'Ukraine',\n",
       " 'Amazonギフト券',\n",
       " 'رمضان_مميز_بايه',\n",
       " 'StandingOnaBox',\n",
       " 'impressionism',\n",
       " 'هدايا_ستايلش',\n",
       " 'DeFi',\n",
       " 'BuildWealth',\n",
       " 'TVD',\n",
       " 'MNTwins',\n",
       " 'BNX',\n",
       " 'carhire',\n",
       " 'WorldPremiere',\n",
       " 'BBN',\n",
       " 'Mayabeque',\n",
       " 'StandUpForUkraine',\n",
       " 'Bitcoin',\n",
       " 'Essay',\n",
       " 'Bajwa',\n",
       " 'Kramatorsk',\n",
       " 'comm_college',\n",
       " '羅德弘',\n",
       " 'CapFoodFight',\n",
       " 'Dogging',\n",
       " 'nj',\n",
       " 'Airdrop',\n",
       " 'ZLAN2022',\n",
       " 'stlblues',\n",
       " 'PP22000093736',\n",
       " 'SexRol',\n",
       " 'avgeeks',\n",
       " 'FuckTexas',\n",
       " 'France',\n",
       " 'YOONJAEHYUK',\n",
       " 'EXO',\n",
       " 'DonBelle',\n",
       " 'UnDíaComoHoy',\n",
       " 'Breakthesilence',\n",
       " 'FerrariDaytonaSP3',\n",
       " 'SmackDown',\n",
       " 'StayAlive',\n",
       " 'Advertising',\n",
       " 'Term',\n",
       " 'attackontitan',\n",
       " 'infinitytrain',\n",
       " 'madrigalcest',\n",
       " 'onlineclass',\n",
       " 'NFTs',\n",
       " 'Crypto',\n",
       " 'sextou',\n",
       " '사딸라',\n",
       " 'EuroLeagueWomen',\n",
       " 'rtitBot',\n",
       " 'workingclass',\n",
       " 'KaoriSakamoto',\n",
       " 'touchdown',\n",
       " 'A320Family',\n",
       " 'rainyandcold',\n",
       " 'GalaxyHeroes',\n",
       " 'XRP',\n",
       " 'apexlegendsporn',\n",
       " 'BSC',\n",
       " 'Pakistan',\n",
       " 'TRX',\n",
       " '70s',\n",
       " 'infosec',\n",
       " 'FreeNnamdikanu',\n",
       " 'VectorStock',\n",
       " 'SB19',\n",
       " 'vietnam',\n",
       " 'WewantOurCountryBack',\n",
       " 'GameFi',\n",
       " 'PlayHardHonorGod',\n",
       " 'AmbedkarJayanti',\n",
       " 'weareallwonders',\n",
       " '原神',\n",
       " 'expertanswers',\n",
       " 'RestoreTheSnyderVerse',\n",
       " 'LEBANON',\n",
       " 'KoiPuchayToKehnaBilawalAyaTha',\n",
       " 'Ethiopian',\n",
       " 'BBMAs',\n",
       " 'Ukraine',\n",
       " 'France',\n",
       " 'birpet',\n",
       " 'TeamPete',\n",
       " 'WPS',\n",
       " 'FolkFestHappy',\n",
       " 'HwaSa',\n",
       " 'BTSV',\n",
       " 'Murphy',\n",
       " 'JohnsonOut75',\n",
       " 'pearson',\n",
       " 'pierre4pm',\n",
       " 'Excalibur',\n",
       " 'themasters',\n",
       " 'OldEnglish',\n",
       " 'ASSE',\n",
       " 'Justiceforbella',\n",
       " 'Ukraine',\n",
       " 'leader',\n",
       " 'MCILIV',\n",
       " 'jungkook_StayAlive',\n",
       " 'AZUKI',\n",
       " 'France',\n",
       " 'BBMAs',\n",
       " 'キンプる',\n",
       " 'Germany',\n",
       " 'DutyFirst',\n",
       " 'V_ChristmasTree',\n",
       " 'anipoke',\n",
       " 'bussy',\n",
       " 'رمضان_مميز_بايه',\n",
       " 'freemint',\n",
       " 'Russian',\n",
       " 'BTS',\n",
       " 'NASCAR',\n",
       " 'TeamStar',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " 'Amazon',\n",
       " 'KLV',\n",
       " 'Call811BeforeYouDig250',\n",
       " 'KAIHO',\n",
       " '승관',\n",
       " 'istandwithbet9ja',\n",
       " 'France',\n",
       " 'MrControversyX',\n",
       " 'EVs',\n",
       " 'ExtremeOvershoot',\n",
       " 'saitama',\n",
       " 'actors',\n",
       " 'WesternTigray',\n",
       " 'AGTG',\n",
       " 'SmackDown',\n",
       " 'SB19',\n",
       " 'alterpinay',\n",
       " 'OurFlagMeansDeath',\n",
       " 'OliviaRodrigo',\n",
       " 'Astrology',\n",
       " 'ship30for30',\n",
       " 'Morena',\n",
       " 'Ukraine',\n",
       " 'ぺこらーと',\n",
       " 'ottawa',\n",
       " 'Bitcoin',\n",
       " 'pilab',\n",
       " 'Tigray',\n",
       " 'SLOPPYTOPPY',\n",
       " 'TheWaitIsOver_BTSinVegas',\n",
       " 'expert',\n",
       " 'auspol',\n",
       " 'CulturaVenezolana',\n",
       " 'ベトナム語',\n",
       " 'SanFrancisco',\n",
       " 'Vancouver',\n",
       " 'Nintendo',\n",
       " 'News',\n",
       " 'Ethiopian',\n",
       " 'ニューギン',\n",
       " 'Binate',\n",
       " 'cloud',\n",
       " 'financialliteracymonth',\n",
       " 'Blaziken',\n",
       " 'PP22000093794',\n",
       " 'ksed',\n",
       " 'Tigray',\n",
       " 'donorschoose',\n",
       " 'TwinPeaks',\n",
       " 'expertanswers',\n",
       " 'StraightUpTX',\n",
       " '新生活',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " 'positivity',\n",
       " 'attackontitan',\n",
       " 'NFTGiveaway',\n",
       " 'DimashConcertGermany',\n",
       " 'noticias',\n",
       " 'Florida',\n",
       " 'TigrayGenocide',\n",
       " 'Tigray',\n",
       " '드루와_드루와_챌린진',\n",
       " 'JO1',\n",
       " 'LigaPro',\n",
       " 'BTSARMY',\n",
       " 'WesternTigray',\n",
       " 'Malakas',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " '佐藤ひびき',\n",
       " 'Ottawa',\n",
       " 'BI',\n",
       " 'ameliaRT',\n",
       " 'NewsNight',\n",
       " 'Ableg',\n",
       " 'WorldHealthDay',\n",
       " 'WritingCommunity',\n",
       " 'DDoS',\n",
       " 'Crypto',\n",
       " 'EddiesGamingNews',\n",
       " 'AboutToThrowHands',\n",
       " 'ポムポムプリンと糖質0ｇ麺チャレンジ',\n",
       " 'TejRan',\n",
       " 'Poshmark',\n",
       " 'GRN',\n",
       " 'V_ChristmasTree',\n",
       " 'DeFi',\n",
       " 'fanart',\n",
       " 'StrengthTraining',\n",
       " 'Prabhas',\n",
       " 'PS5',\n",
       " 'MONEY',\n",
       " 'MarchMadness',\n",
       " 'Ohio',\n",
       " 'BumpInTheRoad',\n",
       " 'Arab',\n",
       " 'Steelers',\n",
       " '도쿄리벤져스',\n",
       " 'TheBlacklist',\n",
       " 'AndNothingForKpop',\n",
       " 'blueycapsules',\n",
       " 'Armenia',\n",
       " 'Funtua',\n",
       " 'RollingStones',\n",
       " 'buytolet',\n",
       " 'Dros22',\n",
       " 'Tunein',\n",
       " 'attackontitan',\n",
       " 'LePen',\n",
       " 'WHEEIN',\n",
       " 'Burnaby',\n",
       " 'eFootball2022',\n",
       " 'Skincare',\n",
       " 'SDGs',\n",
       " 'blog',\n",
       " 'آمریکا_کے_جج_جیت_گئے',\n",
       " 'PBRIsThere',\n",
       " 'Ontario',\n",
       " 'BTS',\n",
       " 'pinupqueen',\n",
       " 'Nebraska',\n",
       " 'LongBeach',\n",
       " 'basketball',\n",
       " 'NewDeal4Animation',\n",
       " 'bbcqt',\n",
       " 'insiderbuying',\n",
       " 'JusticeForBella',\n",
       " 'Crypto',\n",
       " '생각정리',\n",
       " 'FF14',\n",
       " 'GutlessAlbo',\n",
       " 'メンテナンスちゃん応援キャンペーン',\n",
       " 'GodMorningFriday',\n",
       " 'IPCCReport',\n",
       " 'USA',\n",
       " 'DraftKings',\n",
       " 'JusticeForJohnnyDepp',\n",
       " 'NASCAR',\n",
       " 'NASCAR',\n",
       " 'DeFi',\n",
       " 'nerdclips',\n",
       " 'ShortStories',\n",
       " 'gangstalking',\n",
       " 'elite',\n",
       " 'Ramadan2022',\n",
       " 'note',\n",
       " 'Call811BeforeYouDig250',\n",
       " 'bigdata',\n",
       " 'weather',\n",
       " 'EXO',\n",
       " 'NASCAR',\n",
       " '2ndaryLifeChess',\n",
       " 'ASBrS22',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " 'TaylorMade',\n",
       " 'StandUpForUkraine',\n",
       " 'Vtuber',\n",
       " 'Russia',\n",
       " 'BehindYouSkipper',\n",
       " 'KSP2FeatureVideos',\n",
       " 'YamiGautam',\n",
       " 'USA',\n",
       " 'USA',\n",
       " 'FoxNews',\n",
       " 'Giants',\n",
       " 'Australia',\n",
       " 'HIGNFY',\n",
       " 'Ukraine',\n",
       " 'TheMasters',\n",
       " 'gobolt',\n",
       " 'Beast',\n",
       " 'kuromon',\n",
       " 'Binance',\n",
       " 'TAEYONG',\n",
       " 'UCL',\n",
       " 'NFTs',\n",
       " 'FetchAppFriday',\n",
       " 'Cavs',\n",
       " 'devlog',\n",
       " 'Hunter',\n",
       " 'HumanitarianCrisis',\n",
       " 'Ukraine',\n",
       " 'Russia',\n",
       " 'ペッパーレモンチキン',\n",
       " 'NFSCRescue',\n",
       " 'NASCAR',\n",
       " 'Entérate',\n",
       " 'CanCam',\n",
       " 'UFO',\n",
       " 'IndianAct',\n",
       " 'OkState',\n",
       " 'VoteBTS',\n",
       " 'GodMorningSaturday',\n",
       " 'submas',\n",
       " 'TaclobanForBBMSARA',\n",
       " 'ニューギン',\n",
       " 'BlueJays',\n",
       " 'cristianoronaldo',\n",
       " 'FridayThoughts',\n",
       " 'ToryCorruption',\n",
       " 'SupportS3199',\n",
       " 'Football',\n",
       " 'NFA',\n",
       " 'GodMorningFriday',\n",
       " 'BTS',\n",
       " 'education',\n",
       " 'thermally',\n",
       " 'Astros',\n",
       " 'lunatics',\n",
       " 'Peshawar',\n",
       " 'WhatAreWeWaitingFor',\n",
       " 'fatburn',\n",
       " 'ottnews',\n",
       " 'UCL',\n",
       " 'Argentina',\n",
       " 'highered',\n",
       " 'music',\n",
       " 'Cuba',\n",
       " 'PNE',\n",
       " 'sekaigosfb',\n",
       " 'CalBikeSummit',\n",
       " 'Preds',\n",
       " 'Uyghur',\n",
       " 'Jimin',\n",
       " 'Wizardia',\n",
       " 'RBandME',\n",
       " 'TigrayGenocide',\n",
       " 'ConerlyWatch',\n",
       " 'BTS',\n",
       " 'DEVCommunity',\n",
       " '4MyEyesOnly',\n",
       " 'QueenRadio',\n",
       " 'Ethiopian',\n",
       " 'BoCoMo',\n",
       " 'FIND_YOUR_TREASURE',\n",
       " 'くら寿司',\n",
       " 'Mariupol',\n",
       " 'BIGBANG',\n",
       " 'RockChalk',\n",
       " 'HATINC',\n",
       " 'FakeNews',\n",
       " 'NewDeal4Animation',\n",
       " 'JenPsaki',\n",
       " 'NoovoInfo',\n",
       " 'Poshmark',\n",
       " 'BEFIRST',\n",
       " 'Giveaway',\n",
       " 'NeonzNFT',\n",
       " 'Tewatia',\n",
       " 'WesternTigray',\n",
       " 'LNG',\n",
       " 'NASCAR',\n",
       " 'BBMAs',\n",
       " 'BTS',\n",
       " 'KimSooHyun',\n",
       " 'Coachella',\n",
       " 'Prayer',\n",
       " 'DST',\n",
       " 'starwars',\n",
       " 'SmackDown',\n",
       " 'WesternTigray',\n",
       " 'XCLUSIVEINTERVIEW',\n",
       " 'SmackDown',\n",
       " '500DaysOfTigrayGenocide',\n",
       " 'pedos',\n",
       " 'gas',\n",
       " 'CabinetDoor',\n",
       " 'Russia',\n",
       " 'playusssa',\n",
       " 'Quran',\n",
       " 'Tigray',\n",
       " 'Ukraine',\n",
       " 'TexasWarOnWomen',\n",
       " 'shorts',\n",
       " 'SDG3',\n",
       " 'HUENINGKAI',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " 'EXO',\n",
       " 'submas',\n",
       " 'Ethiopia',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " 'PortfolioDay',\n",
       " 'EnergyEfficiency',\n",
       " 'Islamophobia',\n",
       " 'V_ChristmasTree',\n",
       " 'VoteBlue',\n",
       " 'hodl',\n",
       " 'SCOTUS',\n",
       " 'ImranKhan',\n",
       " 'Ukraine',\n",
       " 'GoGreen',\n",
       " 'nft',\n",
       " 'Lenovo',\n",
       " 'painting',\n",
       " '50DaysTill50Years',\n",
       " 'WhatsApp',\n",
       " 'Sales_Representative',\n",
       " 'attackontitan',\n",
       " 'Amazon',\n",
       " 'kickstarter',\n",
       " 'bluejays',\n",
       " 'maxfixit_01',\n",
       " 'ICOMtoken',\n",
       " 'NFT',\n",
       " 'NFTGiveaway',\n",
       " 'Vtuber',\n",
       " 'Ukraine',\n",
       " 'Cuyama',\n",
       " 'health',\n",
       " 'ballbusting',\n",
       " 'sneak',\n",
       " 'SF',\n",
       " 'modified',\n",
       " 'recession',\n",
       " 'attackontitan',\n",
       " 'LongBeach',\n",
       " 'ShakeTheWorld',\n",
       " 'NewDeal4Animation',\n",
       " 'RamazanReminder',\n",
       " 'bkdk',\n",
       " 'Reflections',\n",
       " 'Normas',\n",
       " 'ピザハット',\n",
       " 'FAMILIA',\n",
       " 'VoidGives',\n",
       " 'IPCCReport',\n",
       " 'CaptainSwan',\n",
       " 'RunItBack',\n",
       " 'NumbersProtocol',\n",
       " 'AusGP',\n",
       " 'BTSV',\n",
       " 'CatholicTeachers',\n",
       " 'stocks',\n",
       " 'TigrayGenocide',\n",
       " 'ポムポムプリンと糖質0ｇ麺チャレンジ',\n",
       " 'BTS_Butter',\n",
       " '4MyEyesOnly',\n",
       " 'RussianColonialism',\n",
       " 'BamBam',\n",
       " 'bats',\n",
       " 'okeeffe',\n",
       " 'ScottyTheWorstPrimeMinisterEver',\n",
       " 'StrayKids',\n",
       " 'KimSooHyun',\n",
       " 'Vanguard',\n",
       " 'ตลาดนัดเอนจีน',\n",
       " 'GodMorningSaturday',\n",
       " 'viernes',\n",
       " 'calmdowndonking',\n",
       " 'FSBO',\n",
       " 'nftart',\n",
       " 'LongCovid',\n",
       " 'GoBlue',\n",
       " 'ICYMI',\n",
       " 'DimancheJeVoteMelenchon',\n",
       " 'UndeniablyGorgeousDay',\n",
       " 'UnlimitedTomorrow',\n",
       " 'ミクチャ',\n",
       " 'Tigray',\n",
       " 'PMImranKhan',\n",
       " 'ALOCUCIÓN',\n",
       " 'Justice',\n",
       " 'SoarSuperFalcons',\n",
       " 'werewolf',\n",
       " 'Macron',\n",
       " 'BTS',\n",
       " 'Bears',\n",
       " 'BehindYouSkipperAlways',\n",
       " 'BookReview',\n",
       " 'SobatRupiah',\n",
       " 'امپورٹڈ_گورمنٹ_نامنظور',\n",
       " 'ポムポムプリンと糖質0ｇ麺チャレンジ',\n",
       " 'SouthCarolina',\n",
       " 'Sagittarius',\n",
       " 'DraftKings',\n",
       " 'TestimonyTuesday',\n",
       " 'CryptoNunks',\n",
       " 'recession',\n",
       " 'homeschool',\n",
       " 'SoundCloud',\n",
       " 'SriLanka',\n",
       " '仮想通貨',\n",
       " 'Bitcoin',\n",
       " 'Bindhu',\n",
       " 'Apink',\n",
       " 'KBO',\n",
       " '100DaysOfCode',\n",
       " 'ConerlyWatch',\n",
       " 'PrayerRoom',\n",
       " 'lockdown',\n",
       " 'EXO',\n",
       " 'Libya',\n",
       " 'GalaxyM33',\n",
       " 'Genshinlmpact',\n",
       " 'NowPlaying',\n",
       " 'PTD_ON_STAGE_LV',\n",
       " 'willsmith',\n",
       " 'WritingCommunity',\n",
       " 'Ukulele',\n",
       " 'CELSTJ',\n",
       " 'climate',\n",
       " 'หวานใจมิวกลัฟ',\n",
       " 'Call811BeforeYouDig250',\n",
       " 'photography',\n",
       " 'Crypto',\n",
       " 'RT',\n",
       " 'TwitterWouldBeBetterWithout',\n",
       " 'FridayThoughts',\n",
       " 'Bitcoin',\n",
       " 'KetanjiBrownJackson',\n",
       " 'votefromabroad',\n",
       " 'python',\n",
       " 'ArrestBajrangMuni',\n",
       " 'LeafsForever',\n",
       " 'KDLex',\n",
       " 'CapFoodFight',\n",
       " 'SomosGallosDePelea',\n",
       " 'キンプる',\n",
       " 'Kaeya',\n",
       " 'TaclobanForBBMSARA',\n",
       " 'FreeNnamdikanu',\n",
       " 'WalkOnMemories10thWithEXO',\n",
       " 'EverEarn',\n",
       " 'Breakthesilence',\n",
       " 'kabu',\n",
       " 'newtonianmechanics',\n",
       " 'AMC',\n",
       " 'dermpath',\n",
       " 'エコマナ',\n",
       " 'UnrealEngine',\n",
       " 'PDM',\n",
       " 'SmartNews',\n",
       " 'ColdFury20',\n",
       " 'LadyRaider',\n",
       " 'QUEENDOM2',\n",
       " 'Teilhabe',\n",
       " 'highkill',\n",
       " 'NABJNAHJ22',\n",
       " 'RussianWarCrimes',\n",
       " 'ArrestRajapaksas',\n",
       " 'Bengals',\n",
       " 'Uyghur',\n",
       " 'ComunicadosUG',\n",
       " 'ExpelByrd',\n",
       " 'RDC',\n",
       " 'OurTimexMSS',\n",
       " 'BTS',\n",
       " 'garmin',\n",
       " 'Russia',\n",
       " 'SpeederBot',\n",
       " 'modified',\n",
       " 'Politics',\n",
       " 'Ethiopian',\n",
       " 'AQX',\n",
       " 'Ukrainian',\n",
       " 'アヤカ',\n",
       " 'paydomyhomework',\n",
       " 'GRN',\n",
       " 'ShehbazSharif',\n",
       " 'Israel',\n",
       " 'NFT',\n",
       " 'HeyAmina',\n",
       " '大仏の日',\n",
       " 'original',\n",
       " 'gift',\n",
       " 'bts',\n",
       " 'buhari',\n",
       " 'ポムポムプリンと糖質0ｇ麺チャレンジ',\n",
       " '原神',\n",
       " 'OpenAccess',\n",
       " 'HIGNFY',\n",
       " 'DeFi',\n",
       " 'Radio',\n",
       " 'KoreanPeninsula',\n",
       " 'ideas',\n",
       " 'iMasterNCE',\n",
       " 'CRPWLT',\n",
       " 'BLM',\n",
       " 'explore',\n",
       " 'Genocider_Gotabaya',\n",
       " 'Tigray',\n",
       " 'مسابقة_تالنت_الرمضانية',\n",
       " 'Ethiopian',\n",
       " 'Russia',\n",
       " 'Zero',\n",
       " 'Peshawar',\n",
       " 'pinupqueen',\n",
       " 'USMP',\n",
       " 'SleepFuture',\n",
       " 'fem',\n",
       " 'RebelMoon',\n",
       " 'SriLanka',\n",
       " 'moses',\n",
       " 'ユンホ',\n",
       " 'Dbacks',\n",
       " 'AbsolutelyRemarkable',\n",
       " 'Jeopardy',\n",
       " 'pinupqueen',\n",
       " 'realty',\n",
       " 'ChildrenOfTigray',\n",
       " 'KetanjiBrownJackson',\n",
       " 'Crypto',\n",
       " 'V6',\n",
       " 'ToEvangeliseChrist',\n",
       " 'COVID19',\n",
       " '𝗙𝗟𝗔𝗦𝗛𝗕𝗔𝗖𝗞𝗙𝗥𝗜𝗗𝗔𝗬',\n",
       " 'NEGRO',\n",
       " 'Yours_Jin',\n",
       " 'RussianWarCrimes',\n",
       " 'GetStrongHurtFeelings',\n",
       " 'FNAF',\n",
       " 'hamr',\n",
       " 'Draculaura',\n",
       " 'Mariupol',\n",
       " 'Ethiopia',\n",
       " 'Real_Allah_Is_Kabir',\n",
       " 'キンプる',\n",
       " 'windows10',\n",
       " 'Cornwall',\n",
       " 'royal',\n",
       " 'PMImranKhan',\n",
       " 'Careermobility',\n",
       " 'BBN',\n",
       " 'ad',\n",
       " 'AIRDROP',\n",
       " 'DawgsUp',\n",
       " 'Environment',\n",
       " 'COVID19',\n",
       " 'ไบร์ทวิน',\n",
       " 'HailState',\n",
       " 'eBay',\n",
       " 'YieldFarming',\n",
       " 'oneweek',\n",
       " 'reds',\n",
       " 'Tigray',\n",
       " 'recession',\n",
       " 'AllAboard',\n",
       " 'ImMrMeseeks',\n",
       " 'NYCACC',\n",
       " 'Eastworld',\n",
       " 'Covidvax',\n",
       " 'NFT',\n",
       " 'ISLES',\n",
       " 'BTS_Butter',\n",
       " 'pinupqueen',\n",
       " 'slave',\n",
       " 'TRATONGROUP',\n",
       " 'NewsBreak',\n",
       " 'disgrace',\n",
       " 'submas',\n",
       " 'WeatherbitAPI',\n",
       " 'whitelisting',\n",
       " 'parcl',\n",
       " 'KryvyyRih',\n",
       " 'امپورٹڈ_گورمنٹ_نامنظور',\n",
       " 'シンデレラガールズ',\n",
       " 'postponeneetug2022',\n",
       " 'VegasEd',\n",
       " 'RepublicanExtremists',\n",
       " 'modified',\n",
       " 'Bill15',\n",
       " 'PrayerRoom',\n",
       " 'rocknroll',\n",
       " 'goopfollowgoop',\n",
       " 'SmackDown',\n",
       " 'Wizardia',\n",
       " 'BEFIRST',\n",
       " 'WestPapua',\n",
       " 'Etsy',\n",
       " 'contemporaryart',\n",
       " 'TBT',\n",
       " 'liluzivert',\n",
       " 'health',\n",
       " 'NFTs',\n",
       " 'FuckDCRoundRobin',\n",
       " 'infosec',\n",
       " 'wallet',\n",
       " 'Bitcoin',\n",
       " 'AdventuresOnThe401',\n",
       " 'housingmarket',\n",
       " 'youtube',\n",
       " 'askareej',\n",
       " 'مسابقة_تالنت_الرمضانية',\n",
       " 'Ukraine',\n",
       " 'DragRace',\n",
       " 'eBay',\n",
       " 'Crypto',\n",
       " 'MPSA2022',\n",
       " 'LCState',\n",
       " 'GOT7',\n",
       " 'Tigray',\n",
       " 'physicianassistant',\n",
       " 'TestimonyTuesday',\n",
       " 'PrayerRoom',\n",
       " 'SundayFishSketch',\n",
       " 'NFA',\n",
       " 'DoctorStrange',\n",
       " 'Yemen',\n",
       " 'PulsoLatinoamericano',\n",
       " 'VoteBTS',\n",
       " 'CavsVsNets',\n",
       " 'hamr',\n",
       " 'GIDLE',\n",
       " 'Single',\n",
       " 'แกงกะทิอะไรอร่อยchallenge',\n",
       " 'Time',\n",
       " 'lastnightsoyoumissedit',\n",
       " 'Yield360',\n",
       " 'gayporn',\n",
       " 'PDM',\n",
       " 'albedo',\n",
       " 'JusticeKetanji',\n",
       " 'Ukraine',\n",
       " 'stadia',\n",
       " 'WorldHealthDay2022',\n",
       " 'BusinessProposal',\n",
       " 'Worthy',\n",
       " 'royal',\n",
       " 'bitcoin',\n",
       " 'VTuber',\n",
       " 'SingtoPrachaya',\n",
       " 'AI',\n",
       " 'BTS_THE_CITY_LasVegas',\n",
       " 'V_ChristmasTree',\n",
       " 'blueycapsules',\n",
       " 'MahasiswaBergerak',\n",
       " 'SEC',\n",
       " 'Reflections',\n",
       " 'WhatsApp',\n",
       " 'EXO',\n",
       " 'ToEvangeliseChrist',\n",
       " 'StandWithUkraine',\n",
       " 'Jimin',\n",
       " 'Ramadan',\n",
       " 'Bitcoin',\n",
       " 'Ethiopia',\n",
       " 'Paper',\n",
       " 'MATIC',\n",
       " 'lelelelele',\n",
       " 'Ukraine',\n",
       " 'weather',\n",
       " 'nsfwtwt',\n",
       " 'minneapolis',\n",
       " 'แม้กกี้กู้ดมอนิ่ง',\n",
       " 'follbackNKRI',\n",
       " 'رمضان_مميز_بايه',\n",
       " 'NUFC',\n",
       " '同じ人多いと思う',\n",
       " 'امپورٹڈ_گورمنٹ_نامنظور',\n",
       " 'PublicNudity',\n",
       " 'Anonymous',\n",
       " 'JohnsonOut75',\n",
       " 'SEVENTEEN',\n",
       " 'forextools',\n",
       " 'Poshmark',\n",
       " 'token',\n",
       " 'Beast',\n",
       " 'osun',\n",
       " 'MinerPride',\n",
       " 'WindsorSpitfires',\n",
       " 'FOX13',\n",
       " 'GW2',\n",
       " 'TUTNetwork',\n",
       " 'WeatherbitAPI',\n",
       " 'KoiPuchayToKehnaBilawalAyaTha',\n",
       " 'NFT',\n",
       " 'آمریکا_کے_جج_جیت_گئے',\n",
       " 'GamblingTwitter',\n",
       " 'AirCash2',\n",
       " 'Ottawa',\n",
       " 'CatholicTeachers',\n",
       " 'KosaFridge',\n",
       " 'ココナラ',\n",
       " 'XboxAllAccess',\n",
       " 'NaruSakuMonth2022',\n",
       " 'Amhara',\n",
       " 'sports',\n",
       " 'readingcommunity',\n",
       " 'ToEvangeliseChrist',\n",
       " 'OurFlagMeansDeath',\n",
       " 'punjabpolice',\n",
       " 'ScottyThePathologicalLiar',\n",
       " 'GiuseppeCotellessa',\n",
       " 'キンプる',\n",
       " 'January6th',\n",
       " 'hardcock',\n",
       " 'Newsflash',\n",
       " 'IVE',\n",
       " 'Russia',\n",
       " 'BBMAs',\n",
       " 'ENSD',\n",
       " 'FAMILIA',\n",
       " 'cleaning',\n",
       " 'SchoolStrikeForClimate',\n",
       " 'Linux',\n",
       " 'Avdiivka',\n",
       " 'JohnniesBite',\n",
       " 'BBMAs',\n",
       " 'LadyRaider',\n",
       " 'AGTG',\n",
       " 'TREATCHAERYEONGBETTER',\n",
       " 'WVFlashFic22',\n",
       " 'viraltwitter',\n",
       " 'KAI',\n",
       " 'WesternTigray',\n",
       " 'プレゼント',\n",
       " 'RYR59CQ',\n",
       " 'TheVillages',\n",
       " 'BJP',\n",
       " 'GalilaLeniKikoLa',\n",
       " 'رمضان_مميز_بايه',\n",
       " 'MinerPride',\n",
       " 'attackontitan',\n",
       " 'Auschwitz',\n",
       " 'OurTime_MewSuppasit',\n",
       " 'WesternTigray',\n",
       " 'Bucha',\n",
       " 'fredrickleonard',\n",
       " 'LoveAfterLockup',\n",
       " 'LasVegas',\n",
       " 'HelpTheEarth',\n",
       " 'Hitler',\n",
       " 'Fintech',\n",
       " 'GemAlert',\n",
       " 'ポムポムプリンと糖質0ｇ麺チャレンジ',\n",
       " 'GodMorningSaturday',\n",
       " 'CopaLPF',\n",
       " 'blackpink',\n",
       " '抽選',\n",
       " 'LePen',\n",
       " 'samoyed',\n",
       " 'RollingStones',\n",
       " 'プレモルの最高峰',\n",
       " 'TBT',\n",
       " 'CatholicTeachers',\n",
       " 'USA',\n",
       " 'Peshawar',\n",
       " 'MPSA2022',\n",
       " 'pay',\n",
       " 'OinkOn',\n",
       " 'buyingcontent',\n",
       " 'SmackDown',\n",
       " 'Poshmark',\n",
       " '原神',\n",
       " 'NAZI',\n",
       " 'BTC',\n",
       " 'eaJ',\n",
       " 'tagotee',\n",
       " 'punjabpolice',\n",
       " 'Lockdown',\n",
       " 'AGTG',\n",
       " 'LasVegas',\n",
       " 'ClimateActionScience',\n",
       " 'Tigray',\n",
       " 'Moscow',\n",
       " 'WRDSB',\n",
       " 'libraryweek',\n",
       " 'Cyclones',\n",
       " 'FridayThoughts',\n",
       " 'explore',\n",
       " 'SwachhSurvekshan2022Visakhapatnam',\n",
       " 'BlueJays',\n",
       " 'Ukraine',\n",
       " 'attackontitan',\n",
       " 'TraineeA',\n",
       " 'OKState',\n",
       " 'careforwild',\n",
       " 'ottnews',\n",
       " 'NFA',\n",
       " 'Crypto',\n",
       " 'Ukraine',\n",
       " '便秘',\n",
       " 'TerraformDao',\n",
       " 'ASTRO',\n",
       " 'SCOTUS',\n",
       " 'KaderiminOyunu',\n",
       " 'GIVEAWAY',\n",
       " 'GameFi',\n",
       " 'امپورٹڈ_حکومت_نامنظور',\n",
       " 'Cuba',\n",
       " 'AmharaGenocide',\n",
       " 'HUENINGKAI',\n",
       " 'NKRIPastiPulih',\n",
       " 'flowers',\n",
       " 'ToEvangeliseChrist',\n",
       " 'ENHYPEN',\n",
       " 'Saynotobullyinginmidwifery',\n",
       " 'Russia',\n",
       " 'ASBrS22',\n",
       " 'Svengoolie',\n",
       " 'antlers',\n",
       " 'Web3',\n",
       " 'SexRol',\n",
       " 'TheGreatTranslationMovement',\n",
       " 'Russia',\n",
       " 'Ikenography',\n",
       " 'ANTi',\n",
       " 'FSBO',\n",
       " 'VoteBlue2022',\n",
       " 'metal',\n",
       " 'blackpink',\n",
       " 'VHSL',\n",
       " 'TREASURE',\n",
       " 'NFTartists',\n",
       " 'YoVotoSi',\n",
       " 'Ward6',\n",
       " 'RAKoftheDAY',\n",
       " 'COVAX',\n",
       " 'Ukraine',\n",
       " 'ableg',\n",
       " 'LABV2',\n",
       " 'Yankees',\n",
       " 'RescueAlert',\n",
       " 'TerceraPosicion',\n",
       " 'TransRightsAreHumanRights',\n",
       " 'ToEvangeliseChrist',\n",
       " 'Tigray',\n",
       " 'Pittsburgh',\n",
       " '365DaysOfCode',\n",
       " 'TwitterNatureCommunity',\n",
       " 'StrayKids',\n",
       " 'TraitorTot',\n",
       " 'FursuitFriday',\n",
       " 'langtwt',\n",
       " 'ZinuToken',\n",
       " 'GameFi',\n",
       " 'Amhara',\n",
       " 'Ableg',\n",
       " 'drama',\n",
       " 'voice_over',\n",
       " 'Binance',\n",
       " 'Kramatorsk',\n",
       " 'BTS',\n",
       " 'مسابقة_تالنت_الرمضانية',\n",
       " 'Mekelle',\n",
       " 'sports',\n",
       " 'biotech',\n",
       " 'NFT',\n",
       " 'MTtalk',\n",
       " 'modified',\n",
       " 'EYThakÇÖZÜMşart',\n",
       " 'SB19',\n",
       " 'Hand',\n",
       " 'gagthefag',\n",
       " 'LINEスタンププレミアム',\n",
       " 'Ukraine',\n",
       " 'ArrestBajrangMuni',\n",
       " 'Ontario',\n",
       " 'matsumoto',\n",
       " 'おいしいコーヒーの入れ方',\n",
       " 'jhope',\n",
       " 'TheStage',\n",
       " 'GodMorningFriday',\n",
       " 'ToEvangeliseChrist',\n",
       " ...]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_mod = KeyedVectors.load_word2vec_format('your-vector-file-debiased.kv', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(predict, actual):\n",
    "    result = 0\n",
    "    mag_pre = 1\n",
    "    mag_act = 1\n",
    "    for token in actual:\n",
    "        if predict[0] == token:\n",
    "            result = 1\n",
    "        else:\n",
    "            try:\n",
    "                predict_0 = w2v_mod[predict[0]]\n",
    "                mag_pre = np.linalg.norm(predict_0)\n",
    "            except:\n",
    "                predict_0 = np.zeros(100)\n",
    "                mag_pre = 1\n",
    "            try:\n",
    "                actual_i = w2v_mod[token]\n",
    "                mag_act = np.linalg.norm(actual_i)\n",
    "            except:\n",
    "                actual_i = np.zeros(100)\n",
    "                mag_act = 1\n",
    "            result = max(result, (sum(np.multiply(predict_0, actual_i)) / mag_pre / mag_act))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027795595467179817"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_score = 0\n",
    "for i in range(len(res)):\n",
    "    \n",
    "    avg_score += cos_sim([res[i]], [y_true[i]])\n",
    "avg_score /= len(res)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42e13ebf24d50271801a9661915952d80dc5e0c89fb527b02ba76a07f281f339"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('si630wn22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
